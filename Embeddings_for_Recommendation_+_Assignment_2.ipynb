{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+esx4vSjpJN+yEqhjgzkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaomiKemi/InClassAssignments/blob/main/Embeddings_for_Recommendation_%2B_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "retJVbfwlY13"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "\n",
        "len(df)\n",
        "\n",
        "df.tail(100)\n",
        "\n",
        "user_ids = df[\"userId\"].unique().tolist()\n",
        "movie_ids = df[\"movieId\"].unique().tolist()\n",
        "\n",
        "len(movie_ids)\n",
        "\n",
        "#Non-sequential list of ids\n",
        "movie_ids[:6]\n",
        "\n",
        "#Manually making the dictionary\n",
        "movie_id_to_index = {\n",
        "    31: 1,\n",
        "    1029: 2,\n",
        "    1061: 3\n",
        "}\n",
        "\n",
        "#Use a movie id to look up an index\n",
        "movie_id_to_index[31]\n",
        "\n",
        "#Make a dictionary mapping ids (keys) to indexes (values)\n",
        "user_id_to_index = {x: i for i, x in enumerate(user_ids)}\n",
        "movie_id_to_index = {x: i for i, x in enumerate(movie_ids)}\n",
        "\n",
        "#Make a new column in the dataframe which contains the appropriate index for each user and movie\n",
        "df[\"user_index\"] = [user_id_to_index[i] for i in df[\"userId\"]]\n",
        "df[\"movie_index\"] = [movie_id_to_index[i] for i in df[\"movieId\"]]\n",
        "\n",
        "df.head(5)\n",
        "\n",
        "df[\"rating\"].describe()\n",
        "\n",
        "df[\"rating\"] = MinMaxScaler().fit_transform(df[\"rating\"].values.reshape(-1, 1))\n",
        "\n",
        "df[\"rating\"].describe()\n",
        "\n",
        "#Inputs\n",
        "x = df[[\"user_index\", \"movie_index\"]]\n",
        "#Outputs\n",
        "y = df[\"rating\"]\n",
        "#Get train-test split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "#import library\n",
        "import torch"
      ],
      "metadata": {
        "id": "4yr3cXKEtyoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdd0a15-e0f3-4d20-d1f9-308b0fbcbec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LouisNet(torch.nn.Module):\n",
        "\n",
        "  #Override __init__()\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        print(\"__init__ called\")\n",
        "\n",
        "        #Override forward()\n",
        "    def forward(self, inputs):\n",
        "        print(\"\\nforwards pass (new batch)\")\n",
        "        print(inputs,\"\\n\")\n",
        "        #return the output (its just the input, unchanged)\n",
        "        return inputs\n",
        "\n",
        "        #Make a new instance of LouisNet\n",
        "louisNet = LouisNet()\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "#Fake dataset\n",
        "x = torch.FloatTensor([[1],[2],[3],[4]])\n",
        "y = torch.FloatTensor([[2],[3],[4],[5]])\n",
        "\n",
        "#Do a forwards pass\n",
        "prediction = louisNet(x)\n",
        "loss = loss_fn(prediction, y)\n",
        "\n",
        "class RecommenderNet(torch.nn.Module):\n",
        "    def __init__(self, num_users, num_movies, embedding_size=20):\n",
        "        super().__init__()\n",
        "        self.user_embedding = torch.nn.Embedding(num_users, embedding_size)\n",
        "        self.user_bias = torch.nn.Embedding(num_users, 1)\n",
        "        self.movie_embedding = torch.nn.Embedding(num_movies, embedding_size)\n",
        "        self.movie_bias = torch.nn.Embedding(num_movies, 1)\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #Split out indexes\n",
        "        user_indexes = inputs[:, 0]\n",
        "        movie_indexes = inputs[:, 1]\n",
        "        #Forward pass on embedding layer\n",
        "        user_vector = self.user_embedding(user_indexes)\n",
        "        user_bias = self.user_bias(user_indexes).flatten()\n",
        "        movie_vector = self.movie_embedding(movie_indexes)\n",
        "        movie_bias = self.movie_bias(movie_indexes).flatten()\n",
        "        #Dot product\n",
        "        dot = (user_vector * movie_vector).sum(1)\n",
        "        with_bias = dot + user_bias + movie_bias\n",
        "        #Activation function\n",
        "        output = self.sig(with_bias)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "jHsczoCAsVh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41225ac3-a9fd-4167-802b-dc151dbf8524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__ called\n",
            "\n",
            "forwards pass (new batch)\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [4.]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pick Embedding size\n",
        "EMBEDDING_SIZE = 16\n",
        "#Make new object (calls __init__())\n",
        "num_users = len(user_ids)\n",
        "num_movies = len(movie_ids)\n",
        "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)"
      ],
      "metadata": {
        "id": "qDdqHQvyu9oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#Make a subclass to hold our dataset (movie - user pairs (input) and a rating (label))\n",
        "class MoviesDataset(Dataset):\n",
        "    def __init__(self, X,y):\n",
        "        self.X = torch.IntTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "4mSl0Scnv-HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use our train - validation split to make DataLoader objects\n",
        "train_dl = DataLoader(MoviesDataset(x_train.values,y_train.values), batch_size=64, shuffle=True)\n",
        "validation_dl = DataLoader(MoviesDataset(x_val.values,y_val.values), batch_size=64, shuffle=True)\n",
        "\n",
        "epochs = 10\n",
        "#Use Mean Squared Error as a loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "#Use the Adam algorithm to update the weights based on the loss\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n"
      ],
      "metadata": {
        "id": "Iw7lqRA2wiUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use a for loop to repeat for the desired number of epochs\n",
        "for i in range(epochs):\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    #Use a for loop for each batch (provided by the Dataloader)\n",
        "    running_loss = 0.0\n",
        "    for (index, batch) in enumerate(train_dl):\n",
        "\n",
        "        #Get batch\n",
        "        inputs, labels = batch\n",
        "        model.zero_grad()\n",
        "\n",
        "        #Forward pass\n",
        "        prediction = model(inputs)\n",
        "\n",
        "        #Get Loss\n",
        "        loss = loss_fn(prediction, labels)\n",
        "\n",
        "        #Update weights (back prop)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss\n",
        "\n",
        "    avg_loss = running_loss / (index + 1)\n",
        "\n",
        "    model.train(False)\n",
        "\n",
        "    #Now try with the validation set (no need to update weights, just get loss)\n",
        "    running_vloss = 0.0\n",
        "    for index, vdata in enumerate(validation_dl):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (index + 1)\n",
        "    print('Loss {} Validation Loss {}'.format(avg_loss, avg_vloss))"
      ],
      "metadata": {
        "id": "I1PWrIHEwn-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab14c8a-4e02-4bd8-e868-9485512dc602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.17559689283370972 Validation Loss 0.12125011533498764\n",
            "Loss 0.07939629256725311 Validation Loss 0.08459701389074326\n",
            "Loss 0.049161769449710846 Validation Loss 0.07112640887498856\n",
            "Loss 0.037231411784887314 Validation Loss 0.06597204506397247\n",
            "Loss 0.031229011714458466 Validation Loss 0.06466044485569\n",
            "Loss 0.027740823104977608 Validation Loss 0.06203359737992287\n",
            "Loss 0.025415191426873207 Validation Loss 0.06271978467702866\n",
            "Loss 0.023779675364494324 Validation Loss 0.06316044926643372\n",
            "Loss 0.02260366827249527 Validation Loss 0.06383461505174637\n",
            "Loss 0.021718613803386688 Validation Loss 0.0635344460606575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "num_users, EMBEDDING_SIZE, model.user_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AygNF9W5Q8T",
        "outputId": "1f0b2845-e939-4d07-9075-c99240ad8717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(671, 16, Embedding(671, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the movie data so we can map back to names\n",
        "movie_data = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
        "movie_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfJv9XJ36Zj1",
        "outputId": "55743386-aa1c-402d-b040-fd9e6e18c166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['movieId', 'title', 'genres'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(user = 0, n = 10):\n",
        "    #Get Movie Names\n",
        "    top_n_indexes = get_top_n_indexes(user, n)\n",
        "    top_n = get_names_for_indexes(top_n_indexes)\n",
        "    return top_n\n",
        "\n",
        "def get_names_for_indexes(indexes):\n",
        "    return [movie_data[movie_data[\"movieId\"]==movie_ids[i]][\"title\"].item() for i in indexes]\n",
        "\n",
        "def get_top_n_indexes(user = 0, n = 10):\n",
        "    #For one user, make a pair with every movie index\n",
        "    x = torch.IntTensor([[user, i] for i in np.arange(num_movies)])\n",
        "    #Predict\n",
        "    predicted_ratings = model(x)\n",
        "    #Get Top-N indexes\n",
        "    top_n_indexes = predicted_ratings.argsort()[-n:]\n",
        "    return top_n_indexes"
      ],
      "metadata": {
        "id": "4ith6EUU6pDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random users top 10\n",
        "get_top_n(np.random.randint(num_users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfGaV_VI60Ov",
        "outputId": "aeb94956-f90d-4297-df5d-9580bb8328d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yu-Gi-Oh! (2004)',\n",
              " 'Sympathy for Mr. Vengeance (Boksuneun naui geot) (2002)',\n",
              " 'Witless Protection (2008)',\n",
              " 'Cops (1922)',\n",
              " 'Friends with Money (2006)',\n",
              " 'Crossover (2006)',\n",
              " 'Crimson Pirate, The (1952)',\n",
              " 'All is Bright (2013)',\n",
              " 'Gaslight (1940)',\n",
              " 'Nacho Libre (2006)']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Task 1 - Diversity\n",
        "def calculate_diversity(user_top_10_movies):\n",
        "    similarity_matrix = cosine_similarity(user_top_10_movies)\n",
        "    diversity_matrix = 1 - similarity_matrix\n",
        "    mean_diversity = np.mean(diversity_matrix)\n",
        "    return mean_diversity\n",
        "\n",
        "# Task 1 - Novelty\n",
        "def calculate_novelty(user_top_10_movies, ratings_df):\n",
        "    mean_ratings = ratings_df.loc[user_top_10_movies].mean()\n",
        "    mean_novelty = mean_ratings.mean()\n",
        "    return mean_novelty\n",
        "\n",
        "all_user_top_10_movies = []\n",
        "ratings_df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "\n",
        "diversity_scores = []\n",
        "novelty_scores = []\n",
        "\n",
        "# Calculate diversity and novelty scores for each user's top 10 movies\n",
        "for user_top_10_movies in all_user_top_10_movies:\n",
        "    diversity_scores.append(calculate_diversity(user_top_10_movies))\n",
        "    novelty_scores.append(calculate_novelty(user_top_10_movies, ratings_df))\n",
        "\n",
        "# Calculate mean diversity and novelty for the whole dataset\n",
        "mean_diversity = np.mean(diversity_scores)\n",
        "mean_novelty = np.mean(novelty_scores)\n",
        "\n",
        "print(\"Mean Diversity:\", mean_diversity)\n",
        "print(\"Mean Novelty:\", mean_novelty)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BKZZTBZ7RQu",
        "outputId": "791404ae-29c5-42ee-a607-22fae5e609d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Diversity: nan\n",
            "Mean Novelty: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a movie_embeddings matrix of shape (num_movies, embedding_dimension)\n",
        "movie_titles = [...]  # List of movie titles corresponding to each embedding\n",
        "\n",
        "# Filter for movies with average rating > 5\n",
        "filtered_movies = ratings_df[ratings_df[\"average_rating\"] > 5]\n",
        "filtered_movie_ids = filtered_movies[\"movie_id\"].tolist()\n",
        "\n",
        "# Filter movie_embeddings and movie_titles based on filtered_movie_ids\n",
        "filtered_embeddings = movie_embeddings[filtered_movie_ids]\n",
        "filtered_titles = [movie_titles[i] for i in filtered_movie_ids]\n",
        "\n",
        "# Perform dimensionality reduction using PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(filtered_embeddings)\n",
        "\n",
        "# Plot the top 30 best-rated movies on a 2D graph\n",
        "num_movies_to_plot = 30\n",
        "top_movies_embeddings = reduced_embeddings[:num_movies_to_plot]\n",
        "top_movies_titles = filtered_titles[:num_movies_to_plot]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(top_movies_embeddings[:, 0], top_movies_embeddings[:, 1])\n",
        "\n",
        "# Label each point with the title\n",
        "for i, title in enumerate(top_movies_titles):\n",
        "    plt.annotate(title, (top_movies_embeddings[i, 0], top_movies_embeddings[i, 1]))\n",
        "\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Top 30 Best Rated Movies (PCA)\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F7ImKDKmSVGc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "a9780953-f111-4bbe-b72b-bde87ec183e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'average_rating'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7d97ea178695>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Filter for movies with average rating > 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfiltered_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mratings_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"average_rating\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mfiltered_movie_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_movies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movie_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'average_rating'"
          ]
        }
      ]
    }
  ]
}